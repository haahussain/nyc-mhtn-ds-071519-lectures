{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Off\n",
    "\n",
    "With a partner, answer the following question:\n",
    "\n",
    "Is it legal to scrape data from websites?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Webscraping: How to make sure you don't get blocked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aims:\n",
    "\n",
    "- Write scripts that can handle errors and minimize the likelihood of your IP address getting blocked.\n",
    "\n",
    "- Write a selenium script to automatically log in to websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "- Talk about the legality of scraping\n",
    "- Look at ways to programmatically avoid getting banned\n",
    "- Set up the selenium webdriver\n",
    "- Learn how to use Selenium\n",
    "- Write your own script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url=\"https://www.amazon.com/Best-Sellers/zgbs\"\n",
    "\n",
    "page = requests.get(url)\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = BS(page.content, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls = soup.select('ul#zg_browseRoot a' )\n",
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    page = requests.get(url)\n",
    "#     more code to process the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check 200 status code\n",
    "It is always good to check the HTTP status code earlier and proceed accordingly.\n",
    "\n",
    "This is good:\n",
    "\n",
    "~~~\n",
    "if response.status_code == 200:\n",
    "   #Proceed further\n",
    "~~~\n",
    "\n",
    "This is better:\n",
    "\n",
    "~~~~\n",
    "if response.status_code != 200:\n",
    "  return False\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    page = requests.get(url)\n",
    "    # include code to do status check\n",
    "    if page.status_code != 200:\n",
    "        return page.status_code\n",
    "    \n",
    "    # more code to process the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Never Trust HTML\n",
    "\n",
    "Especially if you can’t control it. Web scraping depends on HTML DOM, a simple change in element or class name could break your entire script. The best way to deal with it is to check if it returns `None`.\n",
    "\n",
    "~~~\n",
    "page_count = soup.select('.pager-pages > li > a')\n",
    "if page_count:\n",
    " #do your stuff\n",
    "else:\n",
    " # ALERT!! Send notification to Admin\n",
    "~~~\n",
    "\n",
    "Here I am checking whether the CSS selector returned something legitimate, if yes then proceed further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    page = requests.get(url)\n",
    "    # include code to do status check\n",
    "    if page.status_code != 200:\n",
    "        return page.status_code\n",
    "    \n",
    "    # more code to process the results\n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    items = soup.select(' .specific_class')\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "    else:\n",
    "        return \"Data is coming back blank\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 .  Set headers\n",
    "\n",
    "`requests` does not force you to use request headers while sending requests, but there are few smart websites that do not let you to get read anything important unless certain headers are not set in it. Once I faced the situation that the HTML I was seeing in browser was different than what I was getting via my script, kind of like magic huh. So, it is always good to make your requests as legitimate as you can. The least you should do is to set a User-Agent.\n",
    "\n",
    "~~~\n",
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "response = requests.get(url, headers=headers, timeout=5)\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "for url in urls:\n",
    "    page = requests.get(url, headers = headers)\n",
    "    # include code to do status check\n",
    "    if page.status_code != 200:\n",
    "        return page.status_code\n",
    "    \n",
    "    # more code to process the results\n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    items = soup.select(' .specific_class')\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "    else:\n",
    "        return \"Data is coming back blank\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set timeout\n",
    "\n",
    "One of the issues with `requests` is that, if you don’t mention **timeout**, it will keep trying until its last breath. This might be good for some certain conditions but not in majority cases. Therefore, it’s always good to set a timeout value for each request. Here I am setting timeout to 5 seconds.\n",
    "\n",
    "~~~\n",
    "response = requests.get(url, headers=headers, timeout=5)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "for url in urls:\n",
    "    page = requests.get(url, headers = headers, timeout=5)\n",
    "    # include code to do status check\n",
    "    if page.status_code != 200:\n",
    "        return page.status_code\n",
    "    \n",
    "    # more code to process the results\n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    items = soup.select(' .specific_class')\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "    else:\n",
    "        return \"Data is coming back blank\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exception handling\n",
    "\n",
    "It is always good to implement exception handling. It does not only help to avoid unexpected exit of script but can also help to log errors and info notification. When using Python requests I prefer to catch exceptions like this:\n",
    "\n",
    "~~~\n",
    "try:\n",
    "    # your logic is here\n",
    "\n",
    "except requests.ConnectionError as e:\n",
    "    print(\"OOPS!! Connection Error. Make sure you are connected to Internet. Technical Details given below.\\n\")\n",
    "    print(str(e))\n",
    "except requests.Timeout as e:\n",
    "    print(\"OOPS!! Timeout Error\")\n",
    "    print(str(e))\n",
    "except requests.RequestException as e:\n",
    "    print(\"OOPS!! General Error\")\n",
    "    print(str(e))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Someone closed the program\") \n",
    "~~~\n",
    "\n",
    "Check the very last one. This one tells the program that if someone wants to terminate program by using Ctrl+C then it wrap things up first and then exist. This situation is good if you are storing information in file and wants to dump all at the time of exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        page = requests.get(url, headers = headers, timeout=5)\n",
    "    # include code to do status check\n",
    "        if page.status_code != 200:\n",
    "            return page.status_code\n",
    "    except requests.ConnectionError as e:\n",
    "        print(\"OOPS!! Connection Error. Make sure you are connected to Internet. Technical Details given below.\\n\")\n",
    "        print(str(e))\n",
    "    except requests.Timeout as e:\n",
    "        print(\"OOPS!! Timeout Error\")\n",
    "        print(str(e))\n",
    "    except requests.RequestException as e:\n",
    "        print(\"OOPS!! General Error\")\n",
    "        print(str(e))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Someone closed the program\") \n",
    "    # more code to process the results\n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    items = soup.select(' .specific_class')\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "    else:\n",
    "        return \"Data is coming back blank\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regulate your request pace\n",
    "\n",
    "Many websites have a limit on how many times you can ping a website within a minute/hour/day. YOu want to be aware of that and change your script in order to account for that.\n",
    "\n",
    "One example is using the `sleep()` function that is a part of the time package.  This can pause your script for a set amount of time.\n",
    "\n",
    "~~~\n",
    "import time\n",
    " \n",
    " \n",
    "## Star loop ##\n",
    "for url in urls:\n",
    "\n",
    "    # try to make resquest here.\n",
    "    \n",
    " \n",
    "    #### Delay for 1 seconds ####\n",
    "    time.sleep(1)\n",
    "        \n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    " \n",
    " \n",
    "## Start loop ##\n",
    "for url in urls:\n",
    "    print(\"Current date & time \" + time.strftime(\"%c\"))\n",
    " \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        page = requests.get(url, headers = headers, timeout=5)\n",
    "    # include code to do status check\n",
    "        if page.status_code != 200:\n",
    "            return page.status_code\n",
    "    except requests.ConnectionError as e:\n",
    "        print(\"OOPS!! Connection Error. Make sure you are connected to Internet. Technical Details given below.\\n\")\n",
    "        print(str(e))\n",
    "    except requests.Timeout as e:\n",
    "        print(\"OOPS!! Timeout Error\")\n",
    "        print(str(e))\n",
    "    except requests.RequestException as e:\n",
    "        print(\"OOPS!! General Error\")\n",
    "        print(str(e))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Someone closed the program\") \n",
    "    # more code to process the results\n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    \n",
    "    items = soup.select(' .specific_class')\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "        \n",
    "    else:\n",
    "        return \"Data is coming back blank\"\n",
    "    \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Save as you go\n",
    "\n",
    "You might run into an issue halfway through your scrape and your script breaks. So you want to make sure you are saving your data as you go.  \n",
    "\n",
    "~~~ \n",
    "import csv\n",
    "...\n",
    "with open(\"~/Desktop/output.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # collected_items = [\n",
    "    #   [\"Product #1\", \"10\", \"http://example.com/product-1\"],\n",
    "    #   [\"Product #2\", \"25\", \"http://example.com/product-2\"],\n",
    "    #   ...\n",
    "    # ]\n",
    "\n",
    "    for item_property_list in collected_items:\n",
    "        writer.writerow(item_property_list)\n",
    "~~~\n",
    "~~~\n",
    "import csv\n",
    "...\n",
    "field_names = [\"Product Name\", \"Price\", \"Detail URL\"]\n",
    "with open(\"~/Desktop/output.csv\", \"w\") as f:\n",
    "    writer = csv.DictWriter(f, field_names)\n",
    "\n",
    "    # collected_items = [\n",
    "    #   {\n",
    "    #       \"Product Name\": \"Product #1\",\n",
    "    #       \"Price\": \"10\",\n",
    "    #       \"Detail URL\": \"http://example.com/product-1\"\n",
    "    #   },\n",
    "    #   ...\n",
    "    # ]\n",
    "\n",
    "    # Write a header row\n",
    "    writer.writerow({x: x for x in field_names})\n",
    "\n",
    "    for item_property_dict in collected_items:\n",
    "        writer.writerow(item_property_dict)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        page = requests.get(url, headers = headers, timeout=5)\n",
    "    # include code to do status check\n",
    "        if page.status_code != 200:\n",
    "            return page.status_code\n",
    "    except requests.ConnectionError as e:\n",
    "        print(\"OOPS!! Connection Error. Make sure you are connected to Internet. Technical Details given below.\\n\")\n",
    "        print(str(e))\n",
    "    except requests.Timeout as e:\n",
    "        print(\"OOPS!! Timeout Error\")\n",
    "        print(str(e))\n",
    "    except requests.RequestException as e:\n",
    "        print(\"OOPS!! General Error\")\n",
    "        print(str(e))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Someone closed the program\") \n",
    "    # more code to process the results\n",
    "    #imagine we have gotten the contents of the page in the soup variable\n",
    "    \n",
    "    items = soup.select(' .specific_class')\n",
    "    if items:\n",
    "        #continue processing the data\n",
    "        \n",
    "    else:\n",
    "        return \"Data is coming back blank\"\n",
    "    #write the line of data to a csv files\n",
    "    with open(\"~/Desktop/output.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    for item_property_list in collected_items:\n",
    "        writer.writerow(item_property_list)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Resources \n",
    "- [More advanced issues](https://blog.hartleybrody.com/web-scraping-cheat-sheet/)\n",
    "- [Request Advanced Usage](http://docs.python-requests.org/en/master/user/advanced/#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping with Python often requires no more than the use of the Beautiful Soup module to reach the goal. Beautiful Soup is a popular Python library that makes web scraping by traversing the DOM (document object model) easier to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium\n",
    "\n",
    "The Selenium package is used to automate web browser interaction from Python. With Selenium, programming a Python script to automate a web browser is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.instagram.com/accounts/login/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = ''\n",
    "pw = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = driver.find_elements_by_css_selector('form input')[0]\n",
    "password = driver.find_elements_by_css_selector('form input')[1]\n",
    "email.send_keys(username)\n",
    "password.send_keys(pw)\n",
    "login = driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[3]/button')\n",
    "login.click()\n",
    "try: \n",
    "    not_now = WebDriverWait(driver, 15).until(\n",
    "        lambda d: d.find_element_by_xpath('//button[text()=\"Not Now\"]')\n",
    "    )\n",
    "    not_now.click()\n",
    "except: \n",
    "    pass\n",
    "driver.get(\"https://www.instagram.com/foodandprobability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transitioning to Beautiful Soup\n",
    "Beautiful Soup remains the best way to traverse the DOM and scrape the data. After utilizing Selenium to handle the interactive parts, it is time to ask Beautiful Soup to grab the data that you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
